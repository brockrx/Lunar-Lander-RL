{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffc564f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import concurrent.futures\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "769ff7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent():\n",
    "    def __init__(\n",
    "        self, \n",
    "        epsilon = 1, \n",
    "        epsilon_decay = 0.999,\n",
    "        epsilon_min = 0.2,\n",
    "        replay_size = 1e6,\n",
    "        batch_size = 2048,\n",
    "        gamma = 0.99,\n",
    "        alpha = 0.001,\n",
    "        layers = [256, 256]\n",
    "        ):\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.env = gym.make(\"LunarLander-v2\")\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.train_initialized = False\n",
    "        \n",
    "        self.memory = ReplayMemory(replay_size)\n",
    "        \n",
    "        self.initialize_q()\n",
    "        \n",
    "    def initialize_q(self):\n",
    "        self.q = NeuralNetwork(8, 4, [256, 256], self.alpha)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "        self.optimizer = torch.optim.Adam(self.q.parameters(), lr=self.alpha)\n",
    "    \n",
    "    def train(self, episodes, eval_interval=100):\n",
    "        if not self.train_initialized:\n",
    "            self.score_list = []\n",
    "            self.values_list = []\n",
    "            self.collect_performance_states()\n",
    "            self.train_initialized = True\n",
    "        #previous_weights = self.q.get_weights().flatten()\n",
    "        for e in range(episodes):\n",
    "            s = self.env.reset()\n",
    "            score = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                a = self.get_action(s, training=True)\n",
    "                s1, r, done, _ = self.env.step(a)\n",
    "                score += r\n",
    "                self.memory.add_transition((s, a, r, s1, int(done)))\n",
    "                self.update()\n",
    "            \n",
    "                s = s1\n",
    "            self.epsilon = max(self.epsilon_decay*self.epsilon, self.epsilon_min)\n",
    "            self.score_list.append(score)\n",
    "            if (e+1)%eval_interval == 0:\n",
    "                value = self.evaluate(100)\n",
    "                self.values_list.append(value)\n",
    "                print(f'{e+1}: {value}')\n",
    "        \n",
    "            \n",
    "    def get_action(self, state, training=True):\n",
    "        if not training:\n",
    "            with torch.no_grad():\n",
    "                action = torch.argmax(self.q.forward(np.expand_dims(state, 0), train=False)[0,:]).item()\n",
    "                return action\n",
    "        val = np.random.random()\n",
    "        if val >= self.epsilon:\n",
    "            with torch.no_grad():\n",
    "                action = torch.argmax(self.q.forward(np.expand_dims(state, 0), train=False)[0,:]).item()\n",
    "        else:\n",
    "            action = np.random.randint(0, high=4)\n",
    "        return action\n",
    "        \n",
    "    def compute_performance_values(self):\n",
    "        with torch.no_grad():\n",
    "            values = self.q.forward(self.performance_states)\n",
    "        mean_value = torch.mean(torch.max(values, axis=1)[0]).item()\n",
    "        return mean_value\n",
    "    \n",
    "    def collect_performance_states(self, episodes=100):\n",
    "        state_list = []\n",
    "        for e in range(episodes):\n",
    "            s = self.env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                state_list.append(s)\n",
    "                a = np.random.randint(0, high=4)\n",
    "                s1, r, done, _ = self.env.step(a)\n",
    "                s = s1\n",
    "        self.performance_states = np.array(state_list)\n",
    "        \n",
    "    def update(self):\n",
    "        \n",
    "        if self.memory.replay_index > self.batch_size or self.memory.replay_full:\n",
    "            data = self.memory.get_replay_sample(self.batch_size)\n",
    "            \n",
    "            # Extract values from data array\n",
    "            s = torch.tensor(data[:,0:8]).float()\n",
    "            a = torch.tensor(np.expand_dims(data[:,8], 1)).to(torch.int64)\n",
    "            r = torch.tensor(np.expand_dims(data[:,9], axis=1))\n",
    "            s1 = torch.tensor(data[:,10:18]).float()\n",
    "            non_term = torch.tensor(np.expand_dims(1 - data[:,18], axis=1))\n",
    "            \n",
    "            # Calculate Target\n",
    "            with torch.no_grad():\n",
    "                v1 = torch.max(self.q.forward(s1), 1, keepdim=True)[0]\n",
    "            target = r + (self.gamma * v1 * non_term)\n",
    "            target = target.float()\n",
    "\n",
    "            # Calculate prediction\n",
    "            self.optimizer.zero_grad()\n",
    "            pred = self.q.forward(s).gather(dim=1, index=a).float()\n",
    "\n",
    "            # Perform gradient descent step\n",
    "            loss = self.loss_fn(pred, target)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            delta = loss.item()\n",
    "        \n",
    "        \n",
    "    def evaluate(self, episodes):\n",
    "        score_arr = np.empty(episodes)\n",
    "        for e in range(episodes):\n",
    "            s = self.env.reset()\n",
    "            score = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                a = self.get_action(s, training=False)\n",
    "                s1, r, done, _ = self.env.step(a)\n",
    "                score += r\n",
    "                s = s1\n",
    "            score_arr[e] = score\n",
    "        return np.mean(score_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58422c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, layer_dims):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.layer_dims = layer_dims\n",
    "        current_dim = input_dim\n",
    "        \n",
    "        self.create_model()\n",
    "        \n",
    "        \n",
    "    def create_model(self):\n",
    "        \n",
    "        self.model = nn.ModuleList()\n",
    "        input_dim = self.input_dim\n",
    "        for layer_dim in self.layer_dims:\n",
    "            self.model.append(nn.Linear(input_dim, layer_dim))\n",
    "            self.model.append(nn.ReLU())\n",
    "            self.model.append(nn.BatchNorm1d(layer_dim))\n",
    "            input_dim = layer_dim\n",
    "        self.model.append(nn.Linear(input_dim, self.output_dim))\n",
    "        \n",
    "    def forward(self, x, train=False):\n",
    "        if type(x) is not torch.Tensor:\n",
    "            x = torch.tensor(x)\n",
    "        if not train:\n",
    "            self.model.eval()\n",
    "        else:\n",
    "            self.model.train()\n",
    "        out = x\n",
    "        for module in self.model:\n",
    "            out = module(out)\n",
    "        return out            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93db02db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    \n",
    "    def __init__(self, size):\n",
    "        self.replay_size = int(size)\n",
    "        self.initialize_replay_memory()\n",
    "    \n",
    "    def initialize_replay_memory(self):\n",
    "        self.memory = np.empty((self.replay_size, 19))\n",
    "        self.replay_index = 0\n",
    "        self.replay_full = False\n",
    "        self.replay_max = 1\n",
    "        \n",
    "    def convert_transition_to_array(self, episode):\n",
    "        transition_array = np.empty(19)\n",
    "        transition_array[0:8] = episode[0]\n",
    "        transition_array[8] = episode[1]\n",
    "        transition_array[9] = episode[2]\n",
    "        transition_array[10:18] = episode[3]\n",
    "        transition_array[18] = episode[4]\n",
    "        return transition_array\n",
    "    \n",
    "    def add_transition(self, episode):\n",
    "        transition_array = self.convert_transition_to_array(episode)\n",
    "        i = self.replay_index\n",
    "        self.memory[i,:] = transition_array\n",
    "        self.replay_index += 1\n",
    "        self.replay_max = min(self.replay_max+1, self.replay_size)\n",
    "        if self.replay_index == self.replay_size:\n",
    "            self.replay_index = 0\n",
    "            self.replay_full = True\n",
    "            \n",
    "    def get_replay_sample(self, samples, index=None):\n",
    "        if index is None:\n",
    "            index = np.random.randint(0,high=self.replay_max, size=samples)\n",
    "        return self.memory[index.reshape(samples,1),np.arange(19).reshape(1,19)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a07f600",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(layers=[256, 256], alpha=0.00001, gamma=0.99, epsilon=1, epsilon_decay=0.995, epsilon_min=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c5f172",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent.train(1000, eval_interval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b883d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "torch.save(agent.q.state_dict(), 'q_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72441a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork(8, 4, [256, 256])\n",
    "model.load_state_dict(torch.load('q_model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1971cfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(episodes, env, model):\n",
    "    score_arr = np.empty(episodes)\n",
    "    for e in range(episodes):\n",
    "        s = env.reset()\n",
    "        score = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = get_action(s, model)\n",
    "            s1, r, done, _ = env.step(a)\n",
    "            score += r\n",
    "            s = s1\n",
    "        score_arr[e] = score\n",
    "    return np.mean(score_arr)\n",
    "\n",
    "def get_action(state, model):\n",
    "    with torch.no_grad():\n",
    "        action = torch.argmax(model.forward(np.expand_dims(state, 0), train=False)[0,:]).item()\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8fe587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_video(env, model, episodes):\n",
    "    for episode in range(episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            a = get_action(s, model)\n",
    "            s1, r, done, _ = env.step(a)\n",
    "            env.render()\n",
    "            time.sleep(0.0001)\n",
    "            s = s1\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae73c0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brock\\anaconda3\\envs\\rldm_env\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:406: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a74f89ebcd72>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMonitor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LunarLander-v2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./video'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mshow_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-7-d756cd3db38f>\u001b[0m in \u001b[0;36mshow_video\u001b[1;34m(env, model, episodes)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mshow_video\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepisode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\rldm_env\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\rldm_env\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[1;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\rldm_env\\lib\\site-packages\\gym\\wrappers\\monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    204\u001b[0m             \u001b[0menabled\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         )\n\u001b[1;32m--> 206\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\rldm_env\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\rldm_env\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[1;34m(self, frame)\u001b[0m\n\u001b[0;32m    160\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_encode_image_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImageEncoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframes_per_sec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_frames_per_sec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoder_version'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\rldm_env\\lib\\site-packages\\gym\\wrappers\\monitoring\\video_recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, output_path, frame_shape, frames_per_sec, output_frames_per_sec)\u001b[0m\n\u001b[0;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ffmpeg'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\"\"Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\"\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDependencyNotInstalled\u001b[0m: Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`."
     ]
    }
   ],
   "source": [
    "env = Monitor(gym.make(\"LunarLander-v2\"), './video', force=True)\n",
    "show_video(env, model, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee9811a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
